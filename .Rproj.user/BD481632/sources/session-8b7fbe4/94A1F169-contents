---
title: "ERD Prediction Models"
author: "Marissa Ashner"
date: "2024-02-14"
output:  
  html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
---


```{r setup, include=FALSE}
# setting global options for code chunks
knitr::opts_chunk$set(echo = TRUE)
```


# Purpose

This script creates the ERD Prediction models for the following outcomes: 

* PROMIS Pain Intensity at 6 months
* PROMIS Pain Interference at 6 months 
* LE PADL at 1 month 
* Step counts at 6 months (not included yet)

```{r}
### ************************************************************************** ###
# Project folders: Stats: '\\duhsnas-pri\dusom_cfa\Private\Statistics\StudiesPro00103483_PRIME KNEE\'
#                  IRB: ' \\duhsnas-pri\dusom_cfa\Private\IRBStudies\StudiesPro00103483_PRIME KNEE\'
# Author: Marissa Ashner
# Created date: February 14, 2024
#
# Modified dates:  
#     - February 28, 2024: Add new variables (bmi, egfr, surgery info)
#                 
# Input:  project folder IRB - data/*
#         
#         
# Output: project folder Stats - output/*
#         project folder IRB - data/Derived/*
# 
### ************************************************************************** ###
```


# Libraries and other setup

```{r echo = FALSE}
##### Packages Needed ##### 
library(dplyr)
library(caret)
library(glmnet)
library(tidyr)
library(mpath)


##### clear existing data and graphics #####  
rm(list=ls())
graphics.off()


##### working directory ##### 

### working directory is project folder Stats/programs/
setwd("//duhsnas-pri/dusom_cfa/Private/Statistics/Studies/Pro00103483_PRIME KNEE/programs/")


# To render this document: 
# rmarkdown::render(input = "30_ERDPredictionModels_20240214.Rmd", output_dir = "../output/", output_file = "30_ERDPredictionModels_20240228.html", clean = FALSE)
```

# Read in Data 

```{r}
### Need the outcome data and the covariate data

erd_outcome_data = readRDS("../../../../IRBstudies/Pro00103483_PRIME KNEE/data/Derived/2_ERDOutcomesAll_20240111.RDS")

erd_covariate_data = readRDS("../../../../IRBstudies/Pro00103483_PRIME KNEE/data/Derived/6_ERDCovariatesACEPull_20240227.RDS")
```

# Select Needed Variables 

```{r}
# From outcomes, need LE PADL 1 month, PROMIS 6 months
### also need baseline 
erd_outcome_data = erd_outcome_data %>% select(subject_id,
                                       pain_intensity_tscore,
                                       pain_interference_tscore,
                                       adl_score_new,
                                       redcap_event_name)

# make wide dataset
erd_outcome_data_wide = erd_outcome_data %>%
  pivot_wider(names_from = redcap_event_name, values_from = c(pain_intensity_tscore, pain_interference_tscore, adl_score_new),
              id_cols = "subject_id") %>% 
  select(subject_id, `pain_intensity_tscore_Baseline`,
         `pain_intensity_tscore_6 Month`, pain_interference_tscore_Baseline,
         `pain_interference_tscore_6 Month`, adl_score_new_Baseline,
         `adl_score_new_1 Month`)

# combine covariates and outcomes 
erd_data = left_join(erd_covariate_data, erd_outcome_data_wide, by = "subject_id")
```

# Missing Indicators 

```{r}
### incorporate specific diseases 
erd_covariate_data = erd_covariate_data %>% mutate(
  vascular = ifelse(vascular_disease == "Yes", spef_vasc %>% as.character(), vascular_disease %>% as.character()) %>% as.factor(),
  heart = ifelse(heart_failure == "Yes", spef_hf %>% as.character(), heart_failure %>% as.character()) %>% as.factor(),
  diab = ifelse(diabetes == "Yes", spef_diab %>% as.character(), diabetes %>% as.character()) %>% as.factor(),
  cancer = ifelse(nonskin_cancer == "Yes", spef_cancer %>% as.character(), nonskin_cancer %>% as.character()) %>% as.factor(),
)

### add NA as factor to factor variables
erd_covariate_data$financial_stress = addNA(erd_covariate_data$financial_stress)
erd_covariate_data$vascular = addNA(erd_covariate_data$vascular)
erd_covariate_data$lung_disease = addNA(erd_covariate_data$lung_disease)
erd_covariate_data$heart = addNA(erd_covariate_data$heart)
erd_covariate_data$diab = addNA(erd_covariate_data$diab)
erd_covariate_data$liver_disease = addNA(erd_covariate_data$liver_disease)
erd_covariate_data$cancer = addNA(erd_covariate_data$cancer)

### add missing indicator for continuous variables 
erd_covariate_data$EstimatedBloodLoss_missing <- ifelse(erd_covariate_data$EstimatedBloodLoss %>% is.na(), 1, 0)
erd_covariate_data$EstimatedBloodLoss[erd_covariate_data$EstimatedBloodLoss %>% is.na()] <- 0

erd_covariate_data$eGFR_missing <- ifelse(erd_covariate_data$eGFR %>% is.na(), 1, 0)
erd_covariate_data$eGFR[erd_covariate_data$eGFR %>% is.na()] <- 0 

erd_covariate_data$age_missing <- ifelse(erd_covariate_data$age %>% is.na(), 1, 0)
erd_covariate_data$age[erd_covariate_data$age %>% is.na()] <- 0 


erd_covariate_data$education_level_missing <- ifelse(erd_covariate_data$education_level %>% is.na(), 1, 0)
erd_covariate_data$education_level[erd_covariate_data$education_level %>% is.na()] <- 0


erd_covariate_data$ms_score_missing <- ifelse(erd_covariate_data$ms_score %>% is.na(), 1, 0)
erd_covariate_data$ms_score[erd_covariate_data$ms_score %>% is.na()] <- 0


erd_covariate_data$dgi_num_completed_missing <- ifelse(erd_covariate_data$dgi_num_completed %>% is.na(), 1, 0)
erd_covariate_data$dgi_num_completed[erd_covariate_data$dgi_num_completed %>% is.na()] <- 0


erd_covariate_data$item_recall_missing <- ifelse(erd_covariate_data$item_recall %>% is.na(), 1, 0)
erd_covariate_data$item_recall[erd_covariate_data$item_recall %>% is.na()] <- 0


erd_covariate_data$trail_b_missing <- ifelse(erd_covariate_data$trail_b %>% is.na(), 1, 0)
erd_covariate_data$trail_b[erd_covariate_data$trail_b %>% is.na()] <- 0


erd_covariate_data$psychosocial_total_new_missing <- ifelse(erd_covariate_data$psychosocial_total_new %>% is.na(), 1, 0)
erd_covariate_data$psychosocial_total_new[erd_covariate_data$psychosocial_total_new %>% is.na()] <- 0

erd_covariate_data$phq9_total_score_missing <- ifelse(erd_covariate_data$phq9_total_score %>% is.na(), 1, 0)
erd_covariate_data$phq9_total_score[erd_covariate_data$phq9_total_score %>% is.na()] <- 0

erd_covariate_data$emotional_support_tscore_missing <- ifelse(erd_covariate_data$emotional_support_tscore %>% is.na(), 1, 0)
erd_covariate_data$emotional_support_tscore[erd_covariate_data$emotional_support_tscore %>% is.na()] <- 0

erd_outcome_data_wide$pain_intensity_tscore_Baseline_missing <- ifelse(erd_outcome_data_wide$pain_intensity_tscore_Baseline %>% is.na(), 1, 0)
erd_outcome_data_wide$pain_intensity_tscore_Baseline[erd_outcome_data_wide$pain_intensity_tscore_Baseline %>% is.na()] <- 0

erd_outcome_data_wide$adl_score_new_Baseline_missing <- ifelse(erd_outcome_data_wide$adl_score_new_Baseline %>% is.na(), 1, 0)
erd_outcome_data_wide$adl_score_new_Baseline[erd_outcome_data_wide$adl_score_new_Baseline %>% is.na()] <- 0

erd_outcome_data_wide$pain_interference_tscore_Baseline_missing <- ifelse(erd_outcome_data_wide$pain_interference_tscore_Baseline %>% is.na(), 1, 0)
erd_outcome_data_wide$pain_interference_tscore_Baseline[erd_outcome_data_wide$pain_interference_tscore_Baseline %>% is.na()] <- 0

### Drop levels for race
erd_covariate_data$race = erd_covariate_data$race %>% droplevels()
```

# Make Dummy Variables

```{r}
# all of the factor variables 
dummyVars_lasso = dummyVars("~ race + ethnicity + gender + 
                      financial_stress + lung_disease + 
                      vascular + heart + 
                      diab + liver_disease + cancer + 
                      AdmissionPatientClass + PrimaryAnesthesiaType", data = erd_covariate_data)
dummyVars_df <- data.frame(predict(dummyVars_lasso, newdata = erd_covariate_data))

# add the continuous variables to the dummyVars
final_variables = cbind(erd_covariate_data %>% 
                          select(subject_id, 
                                 starts_with("education_level"), age, 
                                 total_distance, grip_score, 
                                 starts_with("ms_score"), starts_with("trail_b"), starts_with("item_recall"),
                                 starts_with("dgi_num_completed"), starts_with("psychosocial_total_new"),
                                 starts_with("phq9_total_score"), starts_with("emotional_support_tscore"),
                                 starts_with("eGFR"), starts_with("EstimatedBloodLoss"),
                                 BMI, AnesthesiaTime), 
                        dummyVars_df,
                        erd_outcome_data_wide %>% select(-subject_id))
```


# Run Regressions

## PROMIS Pain Intensity 

```{r}
# remove anyone with missing outcome
final_variables_pain_intensity = final_variables %>% filter(!(is.na(`pain_intensity_tscore_6 Month`)))

# remove those without surgery information (for now)
final_variables_pain_intensity = final_variables_pain_intensity %>% filter(!is.na(BMI))

# save subject ids 
subjects_painintensity = final_variables_pain_intensity %>% 
  select(subject_id, `pain_intensity_tscore_6 Month`)

# define response variable and covariates
response_pain_intensity = final_variables_pain_intensity$`pain_intensity_tscore_6 Month`

covariates_pain_intensity = final_variables_pain_intensity %>% select(-subject_id,
                                            -ends_with("Month"),
                                            -starts_with("adl_score_new_Baseline"),
                                            -starts_with("pain_interference_tscore_Baseline"))
```

### Trying Various alpha with LOO CV

```{r}
cv_lasso_painintensity = data.frame(alpha= c(0, 0.1, 0.3, 0.5, 0.7, 0.9, 1),
                                    mae = 0,
                                    lambda= 0)

for(i in 1:nrow(cv_lasso_painintensity)){
  #perform k-fold cross-validation to find optimal lambda value
cv_model_painintensity_lassocv <- cv.glmnet(covariates_pain_intensity %>% as.matrix(), 
                      response_pain_intensity, 
                      alpha = cv_lasso_painintensity$alpha[i], 
                      type.measure = "mae",
                      nfolds = 10)#nrow(final_variables_pain_intensity))

cv_lasso_painintensity$mae[i] = cv_model_painintensity_lassocv$cvm %>% min()
cv_lasso_painintensity$lambda[i] = cv_model_painintensity_lassocv$lambda.min
}
```


### LASSO with n-1 CV 

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model_painintensity_lassocv <- cv.glmnet(covariates_pain_intensity %>% as.matrix(), 
                      response_pain_intensity, 
                      alpha = 0.1, 
                      type.measure = "mae",
                      nfolds = nrow(final_variables_pain_intensity)-1)

#find optimal lambda value that minimizes test MSE
best_lambda_painintensity_lassocv <- cv_model_painintensity_lassocv$lambda.min

#produce plot of test MSE by lambda value
plot(cv_model_painintensity_lassocv) 

#find coefficients of best model
best_model_painintensity_lassocv <- glmnet(covariates_pain_intensity %>% as.matrix(), 
                     response_pain_intensity, 
                     alpha = 0.5, 
                     lambda = cv_lasso_painintensity$lambda[4])
#plot(best_model, xvar = "lambda")

#use fitted best model to make predictions
y_predicted_painintensity_lassocv <- predict(best_model_painintensity_lassocv,
                                             s = cv_lasso_painintensity$lambda[4],
                                             newx = covariates_pain_intensity %>% as.matrix())

#find SST and SSE
sst <- sum((response_pain_intensity - mean(response_pain_intensity))^2)
sse <- sum((y_predicted_painintensity_lassocv - response_pain_intensity)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_intensity, y_predicted_painintensity_lassocv)
```




```{r}
# coefficients from the model 
coef(best_model_painintensity_lassocv)
```

### Ridge with n-1 CV 

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model_painintensity_ridgecv <- cv.glmnet(covariates_pain_intensity %>% as.matrix(), 
                      response_pain_intensity, 
                      alpha = 0, 
                   #   type.measure = "mae",
                      nfolds = nrow(final_variables_pain_intensity)-1)

#find optimal lambda value that minimizes test MSE
best_lambda_painintensity_ridgecv <- cv_model_painintensity_ridgecv$lambda.min

#produce plot of test MSE by lambda value
plot(cv_model_painintensity_ridgecv) 

#find coefficients of best model
best_model_painintensity_ridgecv <- glmnet(covariates_pain_intensity %>% as.matrix(), 
                     response_pain_intensity, 
                     alpha = 0, 
                     lambda = best_lambda_painintensity_ridgecv)
#plot(best_model, xvar = "lambda")

#use fitted best model to make predictions
y_predicted_painintensity_ridgecv <- predict(best_model_painintensity_ridgecv,
                                             s = best_lambda_painintensity_ridgecv,
                                             newx = covariates_pain_intensity %>% as.matrix())

#find SST and SSE
sst <- sum((response_pain_intensity - mean(response_pain_intensity))^2)
sse <- sum((y_predicted_painintensity_ridgecv - response_pain_intensity)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_intensity, y_predicted_painintensity_ridgecv)

# coefficients from the model 
coef(best_model_painintensity_ridgecv)
```

### LASSO with BIC (no CV)

```{r}
# remove covariates with no variation
covariates_pain_intensity= covariates_pain_intensity %>% select(-cancer.Lung)

fit.lasso_painintensity_lassobic = glmreg(x = covariates_pain_intensity %>% as.matrix(), 
                   y = response_pain_intensity, 
                   alpha = 1,
                   penalty = "enet", 
                   family = "gaussian", 
                   nlambda = 200,
                   trace = FALSE)

num_params = apply(coef(fit.lasso_painintensity_lassobic), 
                   2, function(x) sum(x != 0))
bic = -1*fit.lasso_painintensity_lassobic$twologlik + 2*num_params*log(length(response_pain_intensity))
index = which.min(bic)

best_model_painintensity_lassobic = glmreg(x = covariates_pain_intensity %>% as.matrix(), 
                   y = response_pain_intensity, 
                   alpha = 1,
                   penalty = "enet", 
                   family = "gaussian", 
                   lambda = fit.lasso_painintensity_lassobic$lambda[index],
                   trace = FALSE)

#use fitted best model to make predictions
y_predicted_painintensity_lassobic <- predict(best_model_painintensity_lassobic, newx = covariates_pain_intensity %>% as.matrix())

#find SST and SSE
sst <- sum((response_pain_intensity - mean(response_pain_intensity))^2)
sse <- sum((y_predicted_painintensity_lassobic - response_pain_intensity)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_intensity, y_predicted_painintensity_lassobic)

# coefficients from the model 
coef(best_model_painintensity_lassobic)
```


## PROMIS Pain Interference 

```{r}
# remove anyone with missing outcome
final_variables_pain_interference = final_variables %>% filter(!(is.na(`pain_interference_tscore_6 Month`)))


# remove those without surgery information (for now)
final_variables_pain_interference = final_variables_pain_interference %>% filter(!is.na(BMI))

# save subject ids 
subjects_paininterference = final_variables_pain_interference %>% 
  select(subject_id, `pain_interference_tscore_6 Month`)

# define response variable and covariates
response_pain_interference = final_variables_pain_interference$`pain_interference_tscore_6 Month`

covariates_pain_interference = final_variables_pain_interference %>% select(-subject_id,
                                            -ends_with("Month"),
                                            -starts_with("adl_score_new_Baseline"),
                                            -starts_with("pain_intensity_tscore_Baseline"))
```

### Trying Various alpha with LOO CV

```{r}
cv_lasso_paininterf = data.frame(alpha= c(0, 0.1, 0.3, 0.5, 0.7, 0.9, 1),
                                    mae = 0,
                                    lambda= 0)

for(i in 1:nrow(cv_lasso_paininterf)){
  #perform k-fold cross-validation to find optimal lambda value
cv_model_paininterf_lassocv <- cv.glmnet(covariates_pain_interference %>% as.matrix(), 
                      response_pain_interference, 
                      alpha = cv_lasso_paininterf$alpha[i], 
                      type.measure = "mae",
                      nfolds = nrow(final_variables_pain_interference))#nrow(final_variables_pain_intensity))

cv_lasso_paininterf$mae[i] = cv_model_paininterf_lassocv$cvm %>% min()
cv_lasso_paininterf$lambda[i] = cv_model_paininterf_lassocv$lambda.min
}
```


### LASSO with n-1 CV 


```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model_paininterf_lassocv <- cv.glmnet(covariates_pain_interference %>% as.matrix(), 
                      response_pain_interference, 
                      nfolds = nrow(final_variables_pain_interference),
                      type.measure = "mae",
                      alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda_paininterf_lassocv <- cv_model_paininterf_lassocv$lambda.min

#produce plot of test MSE by lambda value
plot(cv_model_paininterf_lassocv) 

#find coefficients of best model
best_model_paininterf_lassocv <- glmnet(covariates_pain_interference %>% as.matrix(), 
                     response_pain_interference, 
                     alpha = 0.3, 
                     lambda = cv_lasso_paininterf$lambda[3]) #cv_model$lambda[which.min(cv_model$cvm[-1])]) # use second lowest MSE instead

#use fitted best model to make predictions
y_predicted_paininterf_lassocv <- predict(best_model_paininterf_lassocv, 
                                          s = cv_lasso_paininterf$lambda[3],
                                          newx = covariates_pain_interference %>% as.matrix())

#find SST and SSE
sst <- sum((response_pain_interference- mean(response_pain_interference))^2)
sse <- sum((y_predicted_paininterf_lassocv - response_pain_interference)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_interference, y_predicted_paininterf_lassocv)

# coefficients from the model 
coef(best_model_paininterf_lassocv)
```

### Ridge with n-1 CV


```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model_paininterf_ridgecv <- cv.glmnet(covariates_pain_interference %>% as.matrix(), 
                      response_pain_interference, 
                      nfolds = nrow(final_variables_pain_interference)-1,
                      alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda_paininterf_ridgecv <- cv_model_paininterf_ridgecv$lambda.min

#produce plot of test MSE by lambda value
plot(cv_model_paininterf_ridgecv) 

#find coefficients of best model
best_model_paininterf_ridgecv <- glmnet(covariates_pain_interference %>% as.matrix(), 
                     response_pain_interference, 
                     alpha = 0, 
                     lambda = best_lambda_paininterf_ridgecv)

#use fitted best model to make predictions
y_predicted_paininterf_ridgecv <- predict(best_model_paininterf_ridgecv,
                                          s = best_lambda_paininterf_ridgecv,
                                          newx = covariates_pain_interference %>% as.matrix())

#find SST and SSE
sst <- sum((response_pain_interference- mean(response_pain_interference))^2)
sse <- sum((y_predicted_paininterf_ridgecv - response_pain_interference)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_interference, y_predicted_paininterf_ridgecv)

# coefficients from the model 
coef(best_model_paininterf_ridgecv)
```

### Trying various alpha with BIC

```{r}
bic_paininterf = data.frame(alpha= c(0, 0.1, 0.3, 0.5, 0.7, 0.9, 1),
                                    bic=0, 
                                 lambda = 0,
                            index = 0)

for(i in 1:nrow(bic_paininterf)){
fit.lasso_paininterf_lassobic = glmreg(x = covariates_pain_interference %>% as.matrix(), 
                   y = response_pain_interference, 
                   alpha = bic_paininterf$alpha[i],
                   penalty = "enet", 
                   family = "gaussian", 
                   nlambda = 200,
                   trace = FALSE)

num_params = apply(coef(fit.lasso_paininterf_lassobic), 2, function(x) sum(x != 0))
n = length(response_pain_interference)
# bic = -1*fit.lasso_paininterf_lassobic$twologlik + num_params*log(n) #+ 2*num_params*log(n)/n
bic = -1*fit.lasso_paininterf_lassobic$twologlik + 2*num_params + 2*num_params*(num_params+1)/(n-num_params-1)
index = which.min(bic)

bic_paininterf$bic[i] = min(bic)
bic_paininterf$lambda[i] = fit.lasso_paininterf_lassobic$lambda[index]
bic_paininterf$index[i] = index
}
```

### LASSO with BIC (no CV)

```{r}
# remove covariates with no variation
covariates_pain_interference = covariates_pain_interference %>% select(-cancer.Lung)

fit.lasso_paininterf_lassobic = glmreg(x = covariates_pain_interference %>% as.matrix(), 
                   y = response_pain_interference, 
                   alpha = 1,
                   penalty = "enet", 
                   family = "gaussian", 
                   nlambda = 200,
                   trace = FALSE)

num_params = apply(coef(fit.lasso_paininterf_lassobic), 2, function(x) sum(x != 0))
bic = -1*fit.lasso_paininterf_lassobic$twologlik + 2*num_params*log(length(response_pain_interference))
# aic = -1*fit.lasso$twologlik + 2*num_params
index = which.min(bic)

best_model_paininterf_lassobic = glmreg(x = covariates_pain_interference %>% as.matrix(), 
                   y = response_pain_interference, 
                   alpha = 0.5,
                   penalty = "enet", 
                   family = "gaussian", 
                   lambda = bic_paininterf$lambda[4],
                   trace = FALSE)

#use fitted best model to make predictions
y_predicted_paininterf_lassobic <- predict(best_model_paininterf_lassobic,
                                           newx = covariates_pain_interference %>% as.matrix(), 
                       type= "response")

#find SST and SSE
sst <- sum((response_pain_interference - mean(response_pain_interference))^2)
sse <- sum((y_predicted_paininterf_lassobic - response_pain_interference)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(response_pain_interference, y_predicted_paininterf_lassobic)

# coefficients from the model 
coef(best_model_paininterf_lassobic)
```


## LE PADL

Zero-Inflated Negative Binomial  

```{r}
# remove those with missing outcome, and variable(s) with no variation in this sample
final_variables_adl = final_variables %>% filter(!(is.na(`adl_score_new_1 Month`))) 

# remove those without surgery information (for now)
final_variables_adl = final_variables_adl %>% filter(!is.na(BMI))

# save subject ids 
subjects_adl = final_variables_adl %>% 
  select(subject_id, `adl_score_new_1 Month`)

# select needed columns 
final_variables_adl = final_variables_adl %>%
  select(-subject_id, -starts_with("pain"), -diab.Unspecified)
colnames(final_variables_adl)[80] <- "outcome"
```

### LASSO with BIC (no CV) 

```{r}
fit.lasso_adl_lassobic <- zipath(formula = outcome ~.|., 
                    data = final_variables_adl, 
                    family = "negbin", 
                    nlambda = 100,
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 1,
                    alpha.zero = 1)

index = which.min(fit.lasso_adl_lassobic$bic)

#find coefficients of best model
best_model_adl_lassobic <- zipath(outcome ~.|., data = final_variables_adl, 
                    family = "negbin", 
                    lambda.count = fit.lasso_adl_lassobic$lambda.count[index],
                    lambda.zero = fit.lasso_adl_lassobic$lambda.zero[index],
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 1,
                    alpha.zero = 1)

#use fitted best model to make predictions
y_predicted_adl_lassobic <- best_model_adl_lassobic$fitted.values

#find SST and SSE
sst <- sum((final_variables_adl$outcome - mean(final_variables_adl$outcome))^2)
sse <- sum((y_predicted_adl_lassobic - final_variables_adl$outcome)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(final_variables_adl$outcome, y_predicted_adl_lassobic)

# coefficients from the model 
coef(best_model_adl_lassobic)
```

### Ridge with BIC (no CV) 

```{r}
fit.ridge_adl_ridgebic <- zipath(formula = outcome ~.|., 
                    data = final_variables_adl, 
                    family = "negbin", 
                    nlambda = 100,
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 0.001, # using zero was leading to errors...
                    alpha.zero = 0.001)

index = which.min(fit.ridge_adl_ridgebic$bic)

#find coefficients of best model
best_model_adl_ridgebic <- zipath(outcome ~.|., data = final_variables_adl, 
                    family = "negbin", 
                    lambda.count = fit.ridge_adl_ridgebic$lambda.count[index],
                    lambda.zero = fit.ridge_adl_ridgebic$lambda.zero[index],
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 0.001,
                    alpha.zero = 0.001)

#use fitted best model to make predictions
y_predicted_adl_ridgebic <- best_model_adl_ridgebic$fitted.values

#find SST and SSE
sst <- sum((final_variables_adl$outcome - mean(final_variables_adl$outcome))^2)
sse <- sum((y_predicted_adl_ridgebic - final_variables_adl$outcome)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(final_variables_adl$outcome, y_predicted_adl_ridgebic)

# coefficients from the model 
coef(best_model_adl_ridgebic)
```

### LASSO with AIC (no CV) 

```{r}
fit.lasso_adl_lassoaic <- zipath(formula = outcome ~.|., 
                    data = final_variables_adl, 
                    family = "negbin", 
                    nlambda = 100,
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 1,
                    alpha.zero = 1)

index = which.min(fit.lasso_adl_lassoaic$aic)

#find coefficients of best model
best_model_adl_lassoaic <- zipath(outcome ~.|., data = final_variables_adl, 
                    family = "negbin", 
                    lambda.count = fit.lasso_adl_lassoaic$lambda.count[index],
                    lambda.zero = fit.lasso_adl_lassoaic$lambda.zero[index],
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 1,
                    alpha.zero = 1)

#use fitted best model to make predictions
y_predicted_adl_lassoaic <- best_model_adl_lassoaic$fitted.values

#find SST and SSE
sst <- sum((final_variables_adl$outcome - mean(final_variables_adl$outcome))^2)
sse <- sum((y_predicted_adl_lassoaic - final_variables_adl$outcome)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(final_variables_adl$outcome, y_predicted_adl_lassoaic)

# coefficients from the model 
coef(best_model_adl_lassoaic)
```

### Ridge with AIC (no CV) 

```{r}
fit.ridge_adl_ridgeaic <- zipath(formula = outcome ~.|., 
                    data = final_variables_adl, 
                    family = "negbin", 
                    nlambda = 100,
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 0.001, # using zero was leading to errors...
                    alpha.zero = 0.001)

index = which.min(fit.ridge_adl_ridgeaic$aic)

#find coefficients of best model
best_model_adl_ridgeaic <- zipath(outcome ~.|., data = final_variables_adl, 
                    family = "negbin", 
                    lambda.count = fit.ridge_adl_ridgeaic$lambda.count[index],
                    lambda.zero = fit.ridge_adl_ridgeaic$lambda.zero[index],
                    trace =FALSE,
                    penalty = "enet", 
                    alpha.count = 0,
                    alpha.zero = 0)

#use fitted best model to make predictions
y_predicted_adl_ridgeaic <- best_model_adl_ridgeaic$fitted.values

#find SST and SSE
sst <- sum((final_variables_adl$outcome - mean(final_variables_adl$outcome))^2)
sse <- sum((y_predicted_adl_ridgeaic - final_variables_adl$outcome)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

# see observed vs. predicted 
plot(final_variables_adl$outcome, y_predicted_adl_ridgeaic)

# coefficients from the model 
coef(best_model_adl_ridgeaic)
```

# Calculating the ERD 

Calculate the ERD using the final models. 
* Pain Intensity -- LASSO with BIC 
* Pain Interference -- LASSO with BIC 
* LE PADL -- LASSO with AIC 

```{r}

ERD_painintensity = subjects_painintensity %>% 
  mutate(y_predicted_lassobic = y_predicted_painintensity_lassobic %>% as.numeric(), 
         y_predicted_lassocv = y_predicted_painintensity_lassocv %>% as.numeric,
         y_predicted_ridgecv = y_predicted_painintensity_ridgecv %>% 
           as.numeric(),
         ERD_painintensity_lassobic = `pain_intensity_tscore_6 Month` - y_predicted_lassobic,
         ERD_painintensity_lassocv = `pain_intensity_tscore_6 Month` - y_predicted_lassocv,
         ERD_painintensity_ridgecv = `pain_intensity_tscore_6 Month` - y_predicted_ridgecv)

ERD_paininterference = subjects_paininterference %>% 
  mutate(y_predicted_lassobic = y_predicted_paininterf_lassobic %>% as.numeric(), 
         y_predicted_lassocv = y_predicted_paininterf_lassocv %>% as.numeric,
         y_predicted_ridgecv = y_predicted_paininterf_ridgecv %>% 
           as.numeric(),
         ERD_paininterference_lassobic = `pain_interference_tscore_6 Month` - y_predicted_lassobic,
         ERD_paininterference_lassocv = `pain_interference_tscore_6 Month` - y_predicted_lassocv,
         ERD_paininterference_ridgecv = `pain_interference_tscore_6 Month` - y_predicted_ridgecv)

ERD_lepadl = subjects_adl %>% 
  mutate(y_predicted_lassoaic = y_predicted_adl_lassoaic %>% as.numeric(), 
         y_predicted_lassobic = y_predicted_adl_lassobic %>% as.numeric(), 
         y_predicted_ridgeaic = y_predicted_adl_ridgeaic %>% as.numeric(), 
         y_predicted_ridgebic = y_predicted_adl_ridgebic %>% as.numeric(), 
         ERD_lepadl_lassoaic = `adl_score_new_1 Month` - y_predicted_lassoaic, 
         ERD_lepadl_lassobic = `adl_score_new_1 Month` - y_predicted_lassobic, 
         ERD_lepadl_ridgeaic = `adl_score_new_1 Month` - y_predicted_ridgeaic, 
         ERD_lepadl_ridgebic = `adl_score_new_1 Month` - y_predicted_ridgebic)

ERD_all = full_join(ERD_lepadl %>% select(subject_id, ERD_lepadl), 
                    ERD_painintensity %>% select(subject_id, 
                                                 ERD_painintensity), 
                    by = "subject_id") %>% 
  full_join(ERD_paininterference %>% select(subject_id, 
                                            ERD_paininterference), 
            by = "subject_id") %>% 
  mutate(scaled_lepadl = scale(ERD_lepadl) %>% as.numeric(),
         scaled_painintensity = scale(ERD_painintensity) %>% 
           as.numeric(),
         scaled_paininterference = scale(ERD_paininterference) %>% as.numeric())

testing = ERD_all %>% filter(abs(scaled_lepadl) < 2.5)
```

# Kendall's Tau correlation coefficient 

```{r}
cor_painintensity_paininterf = cor.test(ERD_all$ERD_painintensity, 
                                   ERD_all$ERD_paininterference, 
                                   method = "kendall", 
                                   use = "pairwise.complete.obs")

cor_painintensity_lepadl = cor.test(ERD_all$ERD_painintensity, 
                                   ERD_all$ERD_lepadl, 
                                   method = "kendall", 
                                   use = "pairwise.complete.obs")

cor_lepadl_paininterf = cor.test(ERD_all$ERD_lepadl, 
                                   ERD_all$ERD_paininterference, 
                                   method = "kendall", 
                                   use = "pairwise.complete.obs")

cor_painintensity_paininterf

cor_painintensity_lepadl

cor_lepadl_paininterf


```



# Session information

```{r echo = FALSE, results = 'markup'}
sessionInfo()
```
