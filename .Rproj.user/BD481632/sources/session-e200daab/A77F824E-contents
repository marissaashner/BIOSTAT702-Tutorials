---
title: "Introduction to Applied Bayesian Data Analysis: Lesson 11"
subtitle: "Introduction to Hierarchical Models"
author: "Jesse Troy, PhD"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 3
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```

# Background

This lesson provides a conceptual introduction to hierarchical models from a Bayesian point of view. The dataset on the Cherry Blossom race from the Bayes Rules textbook is used to introduce the basic concepts of hierarchical models. The simplest form of hierarchical model is introduced: the random intercepts model. The more complicated random intercepts and slopes model is introduced in a later lesson. This lesson places a heavier emphasis on explanation and exposition than on implementation through programming in R and RStan. Later modules will cover more detail about how to fit the models and do appropriate diagnostics. 

# Prerequisites For This Module
\begin{enumerate}
  \item Read Chapters 15-17 of the "Bayes Rules!" textbook
\end{enumerate}

# Learning Objectives
After completing this lesson you will be able to:
\begin{enumerate}
  \item Define conceptually what a hierarchical model is and how it differs from non-hierarchical models
  \item Identify scenarios where hierarchical models are needed
  \item Explain the notation used to describe hierarchical models
  \item Describe the random slopes model
  \item Explain intraclass correlation
\end{enumerate}

# Introduction to the Cherry Blossom Race Example

Throughout this lesson we will use data from the Cherry Blossom race that is included with your textbook in chapters 16 and 17. The Cherry Blossom Race is a 10-mile foot race held annually in Washington, DC. The dataset we will analyze includes running times for 36 runners who participated in the race for several consecutive years. The primary research question is whether a runner's age is related to the time it takes them to complete the race.

```{r, warning=FALSE, message=FALSE, results=FALSE, echo=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(tidybayes)
library(broom.mixed)
library(sqldf)

# Load data
data(cherry_blossom_sample)
running <- cherry_blossom_sample %>% 
  select(runner, age, net) %>%
  na.omit()
```

The following is a description of the dataset. Each row in the dataset corresponds to a single race for a runner in a given year. There are 3 columns: a unique identifier for each runner (an integer from 1 to 36), the runner's age at the time of the race (the predictor of interest), and the time it took them to run the race (the outcome variable).

```{r, warning=FALSE, message=FALSE}
str(running)
head(running,3)
```

Each runner completed between 4 and 7 races, with the median number of races per runner being 5.

```{r, warning=FALSE, message=FALSE}
racesPerRunner = sqldf('select runner, count(*) as N 
                       from running 
                       group by runner 
                       order by N')
summary(racesPerRunner,digits=2)
```

The following plot shows the running times in minutes for each runner.

```{r, warning=FALSE, message=FALSE}
ggplot(running, aes(x = runner, y = net)) + 
  geom_point() + 
  xlab("Runner ID Number") + 
  ylab("Running Time (minutes)") +
  ggtitle("Running Times for Cherry Blossom Race Participants")
```

# Notation Used for the Example

The outcome variable in this study is running time in minutes, which is a continuous variable. The primary predictor is age in years and is also continuous. We establish the following notation to describe the example. 

Let $Y_{ij}$ denote the outcome of running time for the $i$-th race run by the $j$-th runner. In the sample we have 36 runners, so $j \in \{1,2,..,36\}$. Since $i$ indexes the races for each runner, it's value ranges from 1 to $n_j$, which is the number of races run by the $j$-th runner. Based on the descriptive analysis above, we know that $n_j \in \{4,5,6,7\}$. Finally, we let $X_{ij}$ be the age (in years) of the $j$-th runner in their $i$-th race.

# Naive Analysis of Age vs. Running Time

The primary research question is whether the runner's age at the time of the race is associated with running time. To answer this question we can first plot the data to visually assess the relationship between running time and age, e.g., to evaluate any linear or non-linear relationships. Second, we could fit a linear regression model to estimate the relationship between age and running time. We will perform both steps simultaneously here for the sake of brevity. 

The following Bayesian linear regression model describes the relationship between age and running time without any recognition that the same runner has been observed at multiple ages. Weakly informative priors are used for the example.

\begingroup
  \begin{align*}
    Y_{ij}|\beta_0,\beta_1,\sigma &\sim N(\mu_i,\sigma^2) \text{ where } \mu_i=\beta_0+\beta_1X_{ij} & \\
    \beta_{0c} &\sim N(0,35^2) & \\
    \beta_1 &\sim N(0,15^2) & \\
    \sigma &\sim Exp(0.072) &
  \end{align*}
\endgroup

\textit{Note that we've skipped all of the model diagnostics here for the sake of brevity.}

```{r, warning=FALSE, message=FALSE, results=FALSE}

# Fit the model using weakly informative priors
complete_pooled_model <- stan_glm(
  net ~ age, 
  data = running, family = gaussian, 
  prior_intercept = normal(0, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 84735)
```

```{r, warning=FALSE, message=FALSE}
# Look at the weakly informative priors that were used 
prior_summary(complete_pooled_model)

# Summarize the posterior distributions for the regression coefficients
model_summary <- tidy(complete_pooled_model, conf.int = TRUE, conf.level = 0.95)
model_summary
  
# Plot the posterior median model
B0 <- model_summary$estimate[1]
B1 <- model_summary$estimate[2]
ggplot(running, aes(x = age, y = net)) + 
  geom_point() + 
  geom_abline(aes(intercept = B0, slope = B1))
```

This model suggests that age is not strongly related to running times.

# Weaknesses of the Naive Model 
The weak association observed between age an running time in the naive model might be misleading because it ignores a potentially important source of variability in running times. That source of variability comes from the changes in running time \textit{within runners as they age}. The following plot of the trend in running times for each of the 36 runners in the sample illustrates this source of variability. The posterior median naive model is overlaid on the plot in blue.

```{r}
# Plot of the posterior median model
ggplot(running, aes(x = age, y = net, group = runner)) + 
  geom_smooth(method = "lm", se = FALSE, color = "black", linewidth = 0.5) + 
  geom_abline(aes(intercept = 75.2, slope = 0.268), color = "blue")
```

Why does the blue line look so different from the individual black lines? 

The naive model, which your textbook refers to as the "complete pooled model", assumes that each observed running time comes from a different runner; i.e., it assumes the observations are statistically independent. Therefore, the only source of variability the complete pooled model includes is the between-runner variability. As we can see in the plot above, there is substantial within-runner variability that the complete pooled model ignores. Thus, the blue line appears to "wash over" the within-runner variability.

# A Hierarchical Model Accounts for Within and Between-Group Variability

The following diagram from your textbook shows the general form of a hierarchical model. Based on this structure, we sample "groups" from the population and take multiple observations of the outcome on each group (the Y's as the bottom row of the diagram).

\begin{figure}[H]
  \includegraphics{hierarchical_model.png}
  \caption{From BayesRules! by Johnson, Ott, and Dogucu. See Figure 15.8. https://www.bayesrulesbook.com}
\end{figure}

In the Cherry Blossom Race example the groups are the runners, and the outcomes are the running times that belong to each runner. Some textbooks refer to the groups in a hierarchical model as "clusters" and  hierarchical models are variously referred to in the literature as multilevel models, mixed effect models (or simply mixed models), or random effects models. 

## Specifying the Hierarchical Model

As you might imagine, since we are calling the model "hierarchical" we will be specifying the model in terms of "layers". In terms the Cherry Blossom Race example, Layer 1 addresses within-runner variability, Layer 2 addresses between-runner variability, and Layer 3 specifies prior distributions for the parameters in Layers 1 and 2. In other words:

\begin{align*}
  &\text{Layer 1: model of running time WITHIN runners.} & \\
  &\text{Layer 2: model for how running time varies BETWEEN runners.} & \\
  &\text{Layer 3: prior distributions for model parameters in Layers 1 and 2.}
\end{align*}

## Layer 1: Within Runner Variability

The first layer of the hierarchical model states that each runner's running times, $Y_{ij}$, are distributed normally around their own mean running time, $\mu_j$, with variance $\sigma_{y}^2$ that is common to all runners. 

$$Y_{ij}|\mu_j,\sigma_y \sim N(\mu_j,\sigma_{y}^2)$$
Importantly, since we are studying the effect of age on running times, we need to specify a model for each runner's mean running time based on their age at the time of each race, $X_{ij}$. Therefore, we will modify our statement of Layer 1 for the current analysis as follows. 

$$Y_{ij}|\beta_{0j},\beta_1,\sigma_y \sim N(\mu_{ij},\sigma_{y}^2) \text{ where } \mu_{ij}=\beta_{0j}+\beta_1X_{ij}$$

Now we have a runner-specific means $\mu_{ij}$ that depend on the runner's age at their $i$-th race. We have specified a linear model for these runner-specific means that includes a runner-specific intercept, $\beta_{0j}$. These runner-specific intercepts are effectively each runner's "baseline speed". The effect of age on running times, $\beta_1$, is assumed to be common across all runners (note the absence of a subscript $j$ in $\beta_1$). Similarly, we assume the within-runner variation in running times, $\sigma_y$, is the same for all runners (again, noting the absence of a subscript $j$ for this global parameter).

## Layer 2: Between Runner Variability

Given how Layer 1 is specified, with each runner having their own intercept $\beta_{0j}$, you might have guessed that the between-runner variability described in Layer 2 will focus on the variation of the runner-specific intercepts. Namely, our model will assume that the runner-specific intercepts vary normally around a mean $\beta_0$ with standard deviation $\sigma_0$.

$$\beta_{0j}|\beta_0,\sigma_0 \sim N(\beta_0,\sigma_{0}^2)$$

Thus, $\beta_0$ is interpreted as the average runner's baseline speed and $\sigma_0$ describes the variability in baseline running speeds from runner to runner.

Figure 17.2 from your textbook illustrates Layer 1 and Layer 2 of the hierarchical model. First, each runner has their own mean model in which they have a unique intercept but all share a common slope for age (Layer 1). Second, the black dots on the plot illustrate that the runner-specific intercepts $\beta_{0j}$ vary around the mean baseline running speed $\beta_0$ with standard deviation $\sigma_0$.

\begin{figure}[H]
  \includegraphics{Figure_17_2.png}
  \caption{From BayesRules! by Johnson, Ott, and Dogucu. See Figure 17.2. https://www.bayesrulesbook.com}
\end{figure}

## Layer 3: Prior Distributions

The 3rd layer of our model will specify prior distributions on the parameters in Layers 1 and 2. These are:

\begingroup
  \begin{align*}
    \beta_0 &- \text{ The speed of the average runner, or the global average intercept across runners } & \\
    \beta_1 &- \text{ The effect of age on running speed } & \\
    \sigma_y &- \text{ The variability in running times within runners } & \\
    \sigma_0 &- \text{ The variability in baseline running speeds between runners } &
  \end{align*}
\endgroup

Since our software, rstanarm, requires us to specify model intercepts at the mean of all other covariates we will specify the mean of $\beta_0$ for runners of the average age in the population of race participants. We will label this centered intercept as $\beta_0c$. So, we will have the following prior distributions with the stated hyperparameters:

\begingroup
  \begin{align*}
    \beta_{0c} &\sim N(m_0,s_{0}^2) & \\
    \beta_1 &\sim N(m_1,s_{1}^2) & \\
    \sigma_y &\sim Exp(l_y) & \\
    \sigma_0 &\sim Exp(l_0) &
  \end{align*}
\endgroup


## Full Statement of the Model
Let $Y_{ij}$ denote the outcome of running time for the $i$-th race run by the $j$-th runner. Let $X_{ij}$ be the age (in years) of the $j$-th runner in their $i$-th race. The hierarchical model for running time based on age is specified as follows.

Layer 1: Regression model WITHIN runner j. 
\begin{align*}
    &Y_{ij}|\beta_{0j},\beta_1,\sigma_y \sim N(\mu_{ij},\sigma_{y}^2) \text{ where } \mu_{ij}=\beta_{0j}+\beta_1X_{ij} & 
\end{align*}

Layer 2: Variability in baseline speeds BETWEEN runners.
\begin{align*}
    &\beta_{0j}|\beta_0,\sigma_0 \sim N(\beta_0,\sigma_{0}^2) &
\end{align*}

Layer 3: Priors on Parameters from Layers 1 and 2.
\begin{align*}
    \beta_{0c} &\sim N(m_0,s_{0}^2) & \\
    \beta_1 &\sim N(m_1,s_{1}^2) & \\
    \sigma_y &\sim Exp(l_y) & \\
    \sigma_0 &\sim Exp(l_0) &
\end{align*}

## An Alternate Formulation

There is an alternate way to conceptualize Layer 2 of the model. That is, we can think of the runner-specific intercepts as adjustment from the overall mean $\beta_0$ like this:

$$\beta_{0j} = \beta_0 + b_{0j}$$

The adjustments $b_{0j}$ are then normally distributed around a mean of 0 with standard deviation $\sigma_0$.

$$b_{0j} \sim N(0,\sigma_{0}^2)$$

In this formulation we label $b_{0j}$ as the \textit{random effect} for each runner and we can re-state the model as follows (note: Layer 3 is the same as it was before; only Layers 1 and 2 have changed).

Layer 1: Regression model WITHIN runner j. 
\begin{align*}
    &Y_{ij}|\beta_{0j},\beta_1,\sigma_y \sim N(\mu_{ij},\sigma_{y}^2) \text{ where } \mu_{ij}=(\beta_0 + b_{0j})+\beta_1X_{ij} & 
\end{align*}

Layer 2: Variability in baseline speeds BETWEEN runners.
\begin{align*}
    &b_{0j}|\sigma_0 \sim N(0,\sigma_{0}^2) &
\end{align*}

Layer 3: Priors on Parameters from Layers 1 and 2.
\begin{align*}
    \beta_{0c} &\sim N(m_0,s_{0}^2) & \\
    \beta_1 &\sim N(m_1,s_{1}^2) & \\
    \sigma_y &\sim Exp(l_y) & \\
    \sigma_0 &\sim Exp(l_0) &
\end{align*}

# Fitting the Hierarchical Model to the Cherry Blossom Data

We will fit the hierarchical model describe above to the Cherry Blossom race dataset using the following priors. We've intentionally skipped the detail on how the priors were determined. That will be covered in more detail in a later lesson. 

\begin{align*}
    \beta_{0c} &\sim N(100,10^2) & \\
    \beta_1 &\sim N(2.5,1^2) & \\
    \sigma_y &\sim Exp(0.072) & \\
    \sigma_0 &\sim Exp(1) &
\end{align*}

```{r, warning=TRUE, message=FALSE, results=FALSE, eval=TRUE}
# This code should make the model fitting procedure run faster
options(mc.cores=parallel::detectCores())

# Skipping the steps that we usually go through to tune the prior and jumping
# directly to the posterior distributions for the regression coefficients.
running_model_1 <- stan_glmer(
  net ~ age + (1 | runner), 
  data = running, family = gaussian,
  prior_intercept = normal(100, 10),
  prior = normal(2.5, 1), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 5000*2, seed = 84735, 
  prior_PD = FALSE)
```

The following Markov chain trace plots illustrate that this model has a total of 40 parameters.

\begin{figure}[H]
  \includegraphics{trace_plots.jpeg}
  \caption{Trace plots for the 40 parameters in the random intercept model for the Cherry Bloosom Race.}
\end{figure}

The parameters are:

\begin{itemize}
    \item \textbf{(Intercept)} = $\beta_0$ = the average runner's baseline speed
    \item \textbf{age} = $\beta_1$ = the increase in running speed associated with a 1-year increase in age (applies to all runners)
    \item \textbf{b[(Intercept) runner:j]} = $b_{0j} = \beta_{0j} - \beta_0$ = the difference between runner j's baseline speed and the average baseline speed of all the runners (the random effects)
    \item \textbf{sigma} = $\sigma_y$ = variability of the running times within runners
    \item \textbf{Sigma[runner:(Intercept),(Intercept)]} = $\sigma_0$ = variability in the baseline running speeds between runners; i.e, variability in the runner-specific intercepts, $\beta_{0j}$
\end{itemize}

# The Primary Target of Inference is the Fixed Effect of Age

We call age a "fixed effect" in this model because it is assumed not to vary across runners. In other words, all runners have the same expected increase in running speeed as they grow older. This age-related increase combines with a given runner's baseline running speed to yield an expected increase for a specific runner. But our main concern in this study isn't anything to do with a specific runner. Rather, we are concerned with \underline{the impact of age for the average runner,} and this is what the posterior distribution for the age coefficient is giving us. Here's a plot of that posterior distribution.

```{r}
mcmc_dens(running_model_1,pars=c("age"))

tidy_summary_1 <- tidy(running_model_1, effects = "fixed",
                       conf.int = TRUE, conf.level = 0.95)
tidy_summary_1
```

Based on the 95\% credible interval, we can say there is a 95\% chance that \textbf{the typical runner's time to complete a race increases between 0.87 to 1.73 minutes per year}. 

Plotting 200 posterior plausible models illustrates that there seems to be a clearly relationship between age and increased running time. The blue line on the plot is the median posterior plausible model. 
```{r}

B0 <- tidy_summary_1$estimate[1]
B1 <- tidy_summary_1$estimate[2]
running %>%
  add_linpred_draws(running_model_1, ndraws = 200, re_formula = NA) %>%
  ggplot(aes(x = age, y = net)) +
    geom_line(aes(y = .linpred, group = .draw), alpha = 0.1) +
    geom_abline(intercept = B0, slope = B1, color = "blue") +
    lims(y = c(75, 110))

```

\textbf{Remember, these are all posterior plausible models for the effect of age on running times for the typical runner}. We are not showing the random slopes here; i.e., this picture does not depict anything about between-runner variability!

# The Benefit of Considering Bewtween-Runner Variability: A Comparison With the Pooled Model

The following table compares the posterior median and 95\% credible interval for the fixed effect of age in the naive pooled model, which ignores the hierarchical structure of our data, with the random slopes model that acknowledges running times are clustered under runners. 

\begin{table}[!h]
\centering
\caption{Posterior distribution of the age effect}
\begin{tabular}{|l|l|l|}
\hline
                   & Median & 95\% Credible Interval   \\ \hline
Pooled model       & 0.27   & -0.61, 1.15              \\ \hline
Hierarchical model & 1.30   & 0.87, 1.73               \\ \hline
\end{tabular}
\end{table}

Since the pooled model treats each observation as statistically independent, it ignores an important source of variability in the data which is the within-runner variability in running times. Ignoring this source of variation makes the model's estimate of the effect of age naive in the sense that it ignores this important nuance in the structure of our data. If we were to rely on the pooled model we would clearly come to the wrong conclusions, i.e. that increasing age is unrelated to running times. This seems counter to intuition! 

On the other hand, the hierarchical model, which does allow for within-runner variability in running times, gives us a very different picture of the relationship between age and running times. This result matches our intuition: that runners tend to slow down as they age. 

But so far we have focused our inference on the effect of age for the "typical" runner. How are the differences in baseline running speeds between the runners informative?

# What About the Random Effects? 

Recall that the random effects, $b_{0j}$, measure how each runner's baseline speed deviates from the mean baseline running speed, $\beta_0$. Therefore, the runner-specific baseline speeds--which we can also refer to as the runner-specific intercepts--are $\beta_{0j}=b_{0j}+\beta_0$. Since our model is Bayesian we have a posterior distribution for each $\beta_{0j}$. Therefore, each runner has a universe of posterior plausible models that are made up of their own posterior plausible baseline running speed and a posterior plausible global effect of age on running speed. For example, the plot below shows 100 posterior plausible models for two individual runners, numbers 4 and 5.

```{r}
# 100 posterior plausible models for runners 4 & 5. See Figure 17.6 
# in the Bayes Rules textbook.
running %>%
  filter(runner %in% c("4", "5")) %>% 
  add_linpred_draws(running_model_1, ndraws = 100) %>%
  ggplot(aes(x = age, y = net)) +
    geom_line(
      aes(y = .linpred, group = paste(runner, .draw), color = runner),
      alpha = 0.1) +
    geom_point(aes(color = runner))
```

The plot below looks at the posterior median model for each of the 36 runners in the dataset, i.e., ($\beta_0 + b_{0j}) + \beta1X_{ij}$. The blue line is the posterior median model for the average runner, i.e., $\beta_0 + \beta_1X_{ij}$.

```{r}
# Posterior summaries of runner-specific intercepts
runner_summaries_1 <- running_model_1 %>%
  spread_draws(`(Intercept)`, b[,runner]) %>% 
  mutate(runner_intercept = `(Intercept)` + b) %>% 
  select(-`(Intercept)`, -b) %>% 
  median_qi(.width = 0.80) %>% 
  select(runner, runner_intercept, .lower, .upper)

# Plot runner-specific models with the global model. See Figure 17.7 in the 
# Bayes Rules textbook.
ggplot(running, aes(y = net, x = age, group = runner)) + 
  geom_abline(data = runner_summaries_1, color = "gray",
              aes(intercept = runner_intercept, slope = B1)) + 
  geom_abline(intercept = B0, slope = B1, color = "blue") + 
  lims(x = c(50, 61), y = c(50, 135))
```

# Intraclass Correlation

Without specifically naming it, we have been discussing the modeling of what statisticians often call "correlated data". In other words, models that are appropriate when the outcomes are correlated with each other due to their clustering within a group. The example of the Cherry Blossom race is a special case of correlated data called \textit{longitudinal} data. The name arises because the outcomes (running times for each race the runner participated in) are measured serially over time. In this case, the grouping factor is the runner and the correlation is among the repeated observations of running time within each runner. 

The concept of correlation should be familiar to you since you've studied Pearson's correlation in the past, which measures the strength of linear association between two random variables. Although similar in concept to Pearson's correlation there are some important differences between ICC and Pearson's correlation that are potentially confusing.

## Defining the ICC Conceptually

When you learned about Pearson's correlation, you learned that this could be used to express the strength of association between \textit{two different measures} on the same person. In other words, the data could be organized into (X,Y) pairs where X was an observation of one variable (e.g., age) and Y was an observation of another variable (e.g., systolic blood pressure) taken \textit{on the same person}.

In contrast to Pearson's correlation, ICC is looking at correlation among \textit{repeated measures of the same thing, taken on the same person}. For example, in the Cherry Bloosom race the outcomes $Y_{ij}$ are the running times for the $i$-th race for the $j$-th runner. So, we can use ICC to express the correlation between all the paired running times for runner $j$. For example, we could ask what the correlation is between $Y_{i=1,j=1}$ and $Y_{i=2,j=1}$ or more generally, $Corr(Y_{ij},Y_{kj})$ where $i \neq k$.

A picture might help. Let's return to the plot of running times for each runner from our descriptive analysis.

```{r, warning=FALSE, message=FALSE}
ggplot(running, aes(x = runner, y = net)) + 
  geom_point() + 
  xlab("Runner ID Number") + 
  ylab("Running Time (minutes)") +
  ggtitle("Running Times for Cherry Blossom Race Participants")
```

A close examination of this picture shows that the dots (which are the observed running times) tend to be very close together for each runner. This suggests that the within-runner correlation is relatively high. The word "relative" is key here. Relative to what? 

While we've noted the dots for each runner are close together, it is also the case that the clusters of dots for each runner are fairly variable in terms of their location along the Y axis. For example, the dots for Runner 12 are mostly low while the dots for Runner 36 are quite high. 

These two observations are key to understanding the ICC. Specifically:

\begin{enumerate}
  \item The ICC measures the strength of association between any pair of outcomes measured on the same runner.
  \item Our notation for the ICC is $Corr(Y_{ij},Y_{kj})$ where $i$ and $k$ are two outcomes measured on the $j$-th person
  \item This correlation is necessarily (based on the picture above) based on both the variation in outcomes within each runner, and the extent of variation between the runners.
\end{enumerate}

It is the last point that often causes confusion. But it is actually quite logical. One way to think about it is that \textit{when most of the total variation in the outcome is arising due to differences \textbf{between} runners, this logically implies that there is a great degree of similarity--or high correlation--of outcomes \textbf{within} runners}.

\textbf{Therefore, the ICC, or correlation within runner, will be high when the proportion of total variance in the outcome is mainly due to differences between runners.}

## Mathematical Definition of the ICC

Recall that in our model statement we used $\sigma_y$ to indicate the variability within each runner, and $\sigma_0$ to indicate the variability between runners. Using the intuitive explanation of ICC above, we can now understand that the ICC is simply the proportion of total variation due to the between-group differences:

$$ICC=\frac{\sigma_0}{\sigma_y + \sigma_0} $$
The ICC for the Cherry Bloosom race example is 87\%, which should align with your intution from the plot of the running times for each runner. In other words, the estimate of 87\% literally means that the majority of the total variation is due to differences between runners and this in turn implies that the correlation of any pair of running times for a given runner is quite high. 

\textit{Note, that since the ICC is aproportion it's theoretical range is between 0 and 1. This differs from Pearson's correlation, which has a theoretical range between -1 and 1.}

## Proof

To understand how the ICC is the proportion of total variance due to between-group differences, we will use the random effects formulation of our model, which looked like the following.

Layer 1: Regression model WITHIN runner j. 
\begin{align*}
    &Y_{ij}|\beta_{0j},\beta_1,\sigma_y \sim N(\mu_{ij},\sigma_{y}^2) \text{ where } \mu_{ij}=(\beta_0 + b_{0j})+\beta_1X_{ij} & 
\end{align*}

Layer 2: Variability in baseline speeds BETWEEN runners.
\begin{align*}
    &b_{0j}|\sigma_0 \sim N(0,\sigma_{0}^2) &
\end{align*}

In this notation we've specified a mean model for the outcomes, i.e., $E[Y_{ij}]=\mu_{ij}$, which is a function of the fixed effect of age $\beta_1$ (common to all runners) and each runner's random variation $b_{0j}$ around the global mean baseline running time, $\beta_0$. 

Instead of writing the mean model we can write the model for the $Y_{ij}$ themselves as follows.

\begin{align}
    Y_{ij} &= (\beta_{0} + \beta_{1}X_{ij}) + b_{0j} + \varepsilon_{ij} & \\
           &= \mu_{ij} + b_{0j} +\varepsilon_{ij} & \\
           &= \mu + b_{0j} +\varepsilon_{ij} & 
\end{align}

In step 1, the outcomes are expressed as the mean model plus an error term, $\varepsilon_{ij}$. In step 2, the part of the mean model that doesn't involve the random effects has been replaced with a constant $\mu_{ij}$ that depends on the age of the runner in the $i$-th race. Since there is no point in distinguishing between constants, we simplify our notation in step 3 to replace $\mu_{ij}$ with $\mu$.

Another crucial element of the specification the model for the $Y_{ij}$ is that the random effects themselves are independent random draws from a normal distribution, and the the error terms are also independent random draws from a normal distribution. We explicitly specified the distribution for the random effects previously, and implicitly specified the distribution of the error term when we talked about variability within runners. They are:

\begin{align*}
    b_{0j} &\sim N(0,\sigma_{0}^2) & \\
    \varepsilon_{ij} &\sim N(0,\sigma_{y}^2) &
\end{align*}

Furthermore, it is important to specify that the random effects $b_{0j}$ are independent of the errors $\varepsilon_{ij}$.

So, the total variation in this model is expressed as:

$$Var(Y_{ij})=\sigma_{y}^2 + \sigma_{0}^2$$

Now, we can post the question: what is the correlation of any pair of outcomes for the same runner? Since correlation is a standardized version of covariance we will start by finding $Cov(Y_{ij},Y_{kj})$ and then showing how this leads us to $Corr(Y_{ij},Y_{kj})$. 

\begin{align}
    Cov(Y_{ij},Y_{kj}) &= Cov(\mu + b_{0j} +\varepsilon_{ij},\mu + b_{0j} +\varepsilon_{kj}) & \\
                       &= Cov(b_{0j} +\varepsilon_{ij},b_{0j} +\varepsilon_{kj}) & \\
                       &= Cov(b_{0j},b_{0j}) + Cov(b_{0j},\varepsilon_{kj}) + Cov(\varepsilon_{ij},b_{0j}) +  Cov(\varepsilon_{ij},\varepsilon_{kj}) & \\
                       &= Cov(b_{0j},b_{0j}) & \\
                       &= Var(b_{0j}) & \\
                       &= \sigma_{0}^2
\end{align}

In (3) we are re-stating the covariance in terms of the models for any pair of outcomes $i$ and $k$ on the same runner $j$. We drop the constant terms $\mu$ from the models in moving from (3) to (4) since the covariance of anything with a constant is zero. To get from (4) to (5) we apply the linearity property of covariance, i.e., $Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)$. Because the random effects $b_{oj}$ are independent of the errors $\varepsilon_{ij}$, and because the errors are independent themselves, we can drop nearly all of the terms in (5) and we are left with (6). Finally, the covariance of anything with itself is simply it's variance.

Thus, so far we have shown the covariance between any pair of running times on the same runner is the variance of the random effects, $\sigma_{0}^2$. How do we get to correlation? We simply apply the definition as follows.

\begin{align}
    Corr(Y_{ij},Y_{kj}) &= \frac{Cov(Y_{ij},Y_{kj})}{\sqrt{Var(Y_{ij})Var(Y_{kj})}} & \\
                        &= \frac{\sigma_{0}^2}{\sqrt{(\sigma_{y}^2 + \sigma_{0}^2)(\sigma_{y}^2 + \sigma_{0}^2)}} & \\
                        &= \frac{\sigma_{0}^2}{\sigma_{y}^2 + \sigma_{0}^2} &
\end{align}

In (9) we are stating the definition of correlation. In (10) we plug in values that we know, i.e., the covariance that we just figured out in the numerator and the total variation of the outcomes in the denominator. In (11) we simplify the denominator.

Therefore, we have shown that the ICC is the proportion of total variance due to variation in baseline running speeds between runners!

## Using ChatGPT to Help

What if you couldn't remember the properties of covariance? Here's an example of how you could have used ChatGPT to help you with the first step of the proof where we found $Cov(Y_{ij},Y_{kj})$.

### Strategy for Prompting ChatGPT

Before prompting any AI you need to know exactly what you're looking for. In this case, the first thing to recognize is that we want to find the following.

$$Cov(Y_{ij},Y_{kj}) = Cov(\mu + b_{0j} +\varepsilon_{ij},\mu + b_{0j} +\varepsilon_{kj})$$. 

One possible strategy is to re-frame the problem in general terms by recognizing we are dealing with a constant, $\mu$, and 3 random variables, $b_{0j}$, $\varepsilon_{ij}$, and $\varepsilon_{kj}$. So, the problem can be restated as follows where C is a constant and D, E, and F are the random variables.

$$Cov(Y_{ij},Y_{kj}) = Cov(C + D + E,C + D + F)$$.

Using the model statement, we know D is independent of E and F, and that E is independent of F. Now we can prompt ChatGPT.

### The Prompt

The following is an example prompt using the strategy described above.

\textit{I'm going to ask you to simplify an expression using the rules of covariance. Let C be a constant. Let D, E, and F be random variables. Let D be statistically independent from E and F. Also let E and F be statistically independent. Simplify the expression for Cov(C+D+E,C+D+F) and show your work.}

### A Response from ChatGPT

The following text was generated by ChatGPT. 

To simplify the expression \( \text{Cov}(C+D+E, C+D+F) \), let's apply the properties and definitions of covariance step by step. The covariance of two variables, \(X\) and \(Y\), is defined as \( \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] \), where \(E[X]\) is the expected value of \(X\).

Given that \(C\) is a constant, and \(D\), \(E\), and \(F\) are random variables with \(D\) being statistically independent from \(E\) and \(F\), and \(E\) and \(F\) also being statistically independent, we can use the following properties:

1. **Covariance of a constant and a random variable**: \( \text{Cov}(C, X) = 0 \) for any random variable \(X\) because the constant does not vary with \(X\).
2. **Linearity of covariance**: \( \text{Cov}(X+Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z) \) and similarly for the second argument.
3. **Covariance of a random variable with itself**: \( \text{Cov}(X, X) = \text{Var}(X) \), the variance of \(X\).
4. **Independence**: If two random variables \(X\) and \(Y\) are independent, then \( \text{Cov}(X, Y) = 0 \).

Let's apply these properties to simplify \( \text{Cov}(C+D+E, C+D+F) \):

\[
\text{Cov}(C+D+E, C+D+F) = \text{Cov}(D+E, D+F)
\]

We eliminate \(C\) using property 1. Now, applying property 2:

\[
\text{Cov}(D+E, D+F) = \text{Cov}(D, D) + \text{Cov}(D, F) + \text{Cov}(E, D) + \text{Cov}(E, F)
\]

Given \(D\) and \(F\) are independent, \( \text{Cov}(D, F) = 0 \), and similarly \( \text{Cov}(E, D) = 0 \) since \(E\) and \(D\) are independent. Also, \( \text{Cov}(E, F) = 0 \) since \(E\) and \(F\) are independent. We are left with:

\[
\text{Cov}(D+E, D+F) = \text{Cov}(D, D)
\]

Which simplifies further to:

\[
\text{Cov}(D+E, D+F) = \text{Var}(D)
\]

Therefore, the simplified expression for \( \text{Cov}(C+D+E, C+D+F) \) is \( \text{Var}(D) \), leveraging the independence of the variables and the properties of covariance.