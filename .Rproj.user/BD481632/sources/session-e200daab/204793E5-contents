---
title: "BIOSTAT 702: Exercise 6"
subtitle: "Simple Linear Regression: Visualization"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: false
    toc: true
    toc_depth: 3
    extra_dependencies: ["color"]
urlcolor: blue
header-includes: 
   - \usepackage{tabularx}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \newcommand{\benum}{\begin{enumerate}}
   - \newcommand{\eenum}{\end{enumerate}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
options(tinytex.verbose = TRUE)

library(dplyr)
```

# Learning Objectives
\benum
\item Practice visualizing data with 1 continuous predictor and one continuous outcome
\item Practice performing simulations to help develop intuition
\item Illustrate actions to take in response to a scatterplot
  
\eenum

# How to Do This Exercise

We recommend that you read this entire document prior to answering any of the questions. If anything is unclear please ask for help from the instructors or TAs before getting started. You are also allowed to ask for help from the instructors or TAs while you are working on the assignment. You may collaborate with your classmates on this assignment—in fact, we encourage this--and use any technology resources available to you, including Internet searches, generative AI tools, etc. However, if you collaborate with others on this assignment please be aware that \textit{you must submit answers to the questions written in your own words. This means that you should not quote phrases from other sources, including AI tools, even with proper attribution.} Although quoting with proper attribution is good scholarly practice, it will be considered failure to follow the instructions for this assignment and you will be asked to revise and resubmit your answer. In this eventuality, points may be deducted in accordance with the grading rubric for this assignment as described below. Finally, you do not need to cite sources that you used to answer the questions for this assignment.


# Grading Rubric

The assignment is worth 40 points (4 points per question). The points for each question are awarded as follows: 3 points for answering all parts of the question and following directions, and 1 point for a correct answer. Partial credit may be awarded at the instructor's discretion.

# Question 1

We will be simulating data from a designed experiment, with 100 patients.  Patient 1 receives X=1, patient 2 receives X=2, …, patient 100 receives X=100.

\benum

\item Simulate a SLR model meeting all its assumptions.  The slope should be 0.5, the intercept should be 2, and the errors should be normal with mean 0 and standard deviation 1.  Set the seed value for the pseudorandom number generator to 1, so that everyone obtains identical results.

```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rnorm(100))
```

\item Create a scatterplot of $Y$ vs $X$. How does it appear? 

```{r}
ggplot(data = data, aes(X, Y)) + 
  geom_point()
```


\item Add the best-fitting regression line to the plot. How good does the fit appear to be? 

```{r}
ggplot(data = data, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")
```

\item Create a Q-Q plot to check the normality of the residuals. Do the residuals appear to be normal?

```{r}
# fit SLR 
lm_obj = lm(Y~X, data = data)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()
```


\item Fit a SLR model and calculate the R-square statistic. What is it ?

```{r}
# output summary from SLR model
summary(lm_obj)
```


**Note:** What you have just created is the ideal situation -- all of the sLR assumptions are met and the regression line fits well. If your scatterplot and Q-Q plot look this good: (1) you should be happy; and (2) the data are probably simulated. Also, the very high R-squared is typical of laboratory data, where you are able to manipulate the experimental conditions to radically decrease the level of noise in the data. 

# Question 2 

Repeat the above analyses, but change the standard deviation to 10. What do you find? 

```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rnorm(100, 0, sd = 10))

# scatter plot
ggplot(data = data, aes(X, Y)) + 
  geom_point()

# append best-fit line
ggplot(data = data, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")

# fit SLR 
lm_obj = lm(Y~X, data = data)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()

# output with R2
summary(lm_obj)
```

# Question 3 

Repeat the above analysis, but change the standard deviation to 100.  What do you find?  Even though we know that the signal in the data still exists, the level of noise is sufficiently high to make it impossible to find.


```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rnorm(100, 0, sd = 100))

# scatter plot
ggplot(data = data, aes(X, Y)) + 
  geom_point()

# append best-fit line
ggplot(data = data, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")

# fit SLR 
lm_obj = lm(Y~X, data = data)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()

# output with R2
summary(lm_obj)
```

# Question 4

Repeat the above analysis, but change the standard deviation to 20.  What do you find?  This is typical of data on humans -- a signal can be discerned, but there's lots of noise as well.  Despite the noise, the Q-Q plot shows a very close approximation to normality.


```{r}
# Set Seed
set.seed(1)

# Simulate Data 
dataN20 = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rnorm(100, 0, sd = 20))

# scatter plot
ggplot(data = dataN20, aes(X, Y)) + 
  geom_point()

# append best-fit line
ggplot(data = dataN20, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")

# fit SLR 
lm_obj_dataN20 = lm(Y~X, data = dataN20)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj_dataN20))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()

# output with R2
summary(lm_obj_dataN20)
```

# Question 5

Repeat the above analysis, with the regression parameters as before, but change the distribution of the error term to 10*K, where K is a random number derived from a t-distribution with 5 degrees of freedom.  Does the scatterplot still look good?  What about the Q-Q plot?  Specifically, the t-distribution has heavier tails than the normal.  How can this be discovered in the Q-Q plot?


```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + 10*rt(100, 5))

# scatter plot
ggplot(data = data, aes(X, Y)) + 
  geom_point()

# append best-fit line
ggplot(data = data, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")

# fit SLR 
lm_obj = lm(Y~X, data = data)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()

# output with R2
summary(lm)
```

# Question 6 

Repeat the above analysis, but change the distribution of the errors to exp(10). How does the shape of the Q-Q plot differ from that of a Q-Q plot with errors that follow a t-distribution?


```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rexp(100, 10))

# scatter plot
ggplot(data = data, aes(X, Y)) + 
  geom_point()

# append best-fit line
ggplot(data = data, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")

# fit SLR 
lm_obj = lm(Y~X, data = data)

# Get Residuals 
resid = data.frame(resid_standard = rstandard(lm_obj))

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()

# output with R2
summary(lm_obj)
```


# Question 7

\benum

\item Return to the N(0,20) case.  There are 2 ways to explore whether the shape of the regression function is linear.  One is to add the best-fitting regression line to the scatterplot and visually check whether the data points are consistently near this line.  (This is a loose description.  Among others, you're checking for outliers, assessing the assumption that the variance of Y is unrelated to X, etc.).  You have already started this process.  Continue by creating a residual plot.  Does it show any pattern, or is it consistent with no pattern (i.e., a line with a slope of 0)?  

```{r}
car::residualPlots(lm_obj_dataN20,
              pch=20, col="gray", type = "rstandard", terms = ~ 1, 
              fitted = T, tests = F, quadratic = F)
```


\item The other way to explore whether the shape of the regression function is linear is to add a smoothed curve to the scatterplot and check whether that smoothed curve looks like a line.  Indeed, if that smoothed curve has a linear term, you can perform a formal statistical test by comparing the fit of the simple linear version with the fit of the more complex and flexible version of the regression function.  This will be illustrated in another exercise.  For now, let's just make the comparison visually.  Take the scatterplot and add a LOESS function.  Visually, how close does the smoothed curve look like a straight line?

```{r}
ggplot(data = dataN20, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "loess")
```


\item Do the same with a spline function.  Are the results similar to using LOESS?

```{r}
ggplot(data = dataN20, aes(X, Y)) + 
  geom_point() + 
  geom_smooth(method= "lm")
## CHANGE TO SPLINES 
```


\eenum

# Question 8

Now, let's create an example where the regression function is correct but the assumption about the error term isn't.  Change the distribution of the error term to $X * N(0,1)$.  Thus, the level of noise should increase as $X$ increases.  Does the scatterplot look like a fan?  What does the residual plot show?  Why isn't the Q-Q plot perfectly consistent with a normal distribution?  In practice, the next step in response to a scatterplot that looks like this might be to research models with a non-constant error term.

```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + X*rnorm(100))

# scatter plot
ggplot(data = data, aes(X, Y)) + 
  geom_point()

# fit SLR 
lm_obj = lm(Y~X, data = data)

# Residual Plot
car::residualPlots(lm_obj,
              pch=20, col="gray", type = "rstandard", terms = ~ 1, 
              fitted = T, tests = F, quadratic = F)

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()
```


# Question 9 

Now, let's create an example where a transformation of either Y or X is needed.  Return to the N(0,20) case, and set $Y^* = Y^2$.  So, we know by construction that a square root transformation will be ideal, although that might or might not be apparent from the data.  What do the various plots show?  You should find that the variability of Y* increases as X increases, although the functional form of the relationship might not be obvious.  However, this scatterplot pattern suggests trying transformations of Y* such as $\log(Y^*)$ and $\sqrt{Y^*}$, since if they work (i.e., fix the functional form and stabilize the variability) you can use a simple model with a constant error term. 

```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 2 + 0.5*X + rnorm(100, 0, sd = 20), 
         Ystar = Y^2)

# scatter plot
ggplot(data = data, aes(X, Ystar)) + 
  geom_point()

# fit SLR 
lm_obj = lm(Ystar~X, data = data)

# Residual Plot
car::residualPlots(lm_obj,
              pch=20, col="gray", type = "rstandard", terms = ~ 1, 
              fitted = T, tests = F, quadratic = F)

# QQ plot
ggplot(resid, aes(sample = resid_standard)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()
```


# Question 10 

Now, let's create an example where the error term is correct but the regression function isn't.  Keep the N(0,20) error, but change the regression function to $Y = 0.1 * (X-50)^2$.  The issue should be clear from simply looking at the scatterplot, but go ahead and apply a loess fit (it should look quadratic), a spline fit (it should look quadratic), and append a best-fitting straight line (it should look awful).  Using this information, as an analyst your first thought would probably be to fit a quadratic function.  

```{r}
# Set Seed
set.seed(1)

# Simulate Data 
data = data.frame(X = seq(1, 100, 1)) %>% 
  mutate(Y = 0.1*(X-50)^2 + rnorm(100, 0, sd = 20))

# scatter plot
ggplot(data = data, aes(X, Ystar)) + 
  geom_point()

### Add other scatterplots 
```


