---
title: "BIOSTAT702 R Tutorial 3: "
subtitle: "Simple Linear Regression"
author: "Marissa Ashner, PhD"
date: "Last Updated: 2024-08-12"
output: 
  html_document:
    theme: cosmo
    fig_width: 7
    fig_height: 5
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}
h1.title{
  font-size: 30px;
  font-weight: bold;
  margin-bottom: 0px;
}
h1 {
  font-size: 20px;
}
h2{
  font-size: 16px;
}
h3.subtitle {
  font-size: 25px;
  font-style: italic;
  margin-top: 3px;
}
h3{
  font-size: 14px;
}
h4{
  font-size: 13px;
}
body, code.r, pre{
  font-size: 12px;
}
```


```{r setup, include=FALSE}
# setting global options for code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE)
```

This tutorial will follow closely with parts of Chapters 4 and 5 in the [textbook](https://bookdown.org/rwnahhas/RMPH/datasumm.html).

# Packages 

The following code loads the packages you need for the tutorial.

```{r, message = FALSE}
## NOTE: If you do not have a package installed, you must install it first
# For example, you can run the following code if you don't have the gtsummary package:
# install.packages("gtsummary")


# for navigating within the R project
library(here)

# for data wrangling 
library(tidyverse)

# for nice-looking tables 
library(gtsummary)

# for splines
library(Hmisc)

# for checking assumptions / diagnostics 
library(car)
library(olsrr)
```

# Resources

Below is a list of resources that may be helpful or provide more information on this topic. 

- [Applet for Least Squares Illustration](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm)
- [Guide to Regression Diagnostics with R](https://sscc.wisc.edu/sscc/pubs/RegDiag-R/index.html)

# Load Data 

There will be two different datasets used for this tutorial. Both are loaded here.

```{r}
# Load Data 
# will be called "nhanes_adult_exam_sub" in your environment 
load(here("data/nhanes1718_adult_fast_sub_rmph.Rdata"))

# rename dataset for convenience 
nhanesf <- nhanes_adult_fast_sub

# will be called "unhdd" in your environment
load(here("data/unhdd2020.rmph.rData"))
```

# Visualize the Data 

From the NHANES dataset, we will investigate how the mean fasting glucose varies with waist circumference (a continuous predictor). The first thing to do is visualize the data.

## Visualizing the Distribution of the Outcome 

```{r, message = FALSE}
ggplot(data = nhanesf) + 
  geom_histogram(aes(LBDGLUSI)) + 
  labs(x = "Fasting Glucose (mmol/L)") + 
  geom_vline(xintercept = mean(nhanesf$LBDGLUSI), color = "red")
```

## Visualizing the Distribution of the Predictor 

```{r, message = FALSE}
ggplot(data = nhanesf) + 
  geom_histogram(aes(BMXWAIST)) + 
  labs(x = "Waist Circumference (cm)") + 
  geom_vline(xintercept = mean(nhanesf$BMXWAIST, na.rm = TRUE), 
             color = "red")
```

## Visualizing the Relationship between the Predictor and Outcome 

We can visualize the relationship between the two variables using a scatterplot. We can then append the best fit line to this scatterplot using the `lm` method. We will cover the details of this method in the next section.

```{r, message = TRUE}
# Plot the scatterplot
ggplot(data = nhanesf) + geom_point(aes(BMXWAIST, LBDGLUSI))

# Plot the scatterplot with an appended best fit line 
ggplot(data = nhanesf, aes(BMXWAIST, LBDGLUSI)) + geom_point() + geom_smooth(method = lm, se = FALSE)
```

### Fitting smooth curves to the scatterplot 

Rather than just appending the best fit line, we can also append smoothers such as a LOESS curve or a spline. Here, we will show an example using a LOESS curve and a restricted cubic spline (RCS). Note there are other types of splines available as well. The RCS function `rcspline.eval()` is in the `Hmisc` package.

```{r}
# plot scatterplot with appended LOESS curve 
ggplot(data = nhanesf, aes(BMXWAIST, LBDGLUSI)) + geom_point() + 
  geom_smooth(method = "loess", se = FALSE) 

# plot scatterplot with appended RCS
ggplot(data = nhanesf, aes(BMXWAIST, LBDGLUSI)) + geom_point() + 
  geom_smooth(method = "lm",
              # look at the arguments for ?rcspline.eval to see how you can change the number and location of knots 
              formula = y ~ rcspline.eval(x),
              se = FALSE)
```


# Fitting the Linear Model 

Aside from using the `lm` method in the `geom_smooth` function to append the best fit line to a scatterplot, we can run the `lm()` function independently to fit a simple linear regression model to the data. The output can be explored by using the `summary()` function. This function outputs the following information: 

-   The function call 
-   Descriptive Statistics regarding the residuals 
-   The coefficients
    -   parameter estimates 
    -   standard errors 
    -   t-test statistic 
    -   p-value for the Wald-type t-test
-   The residual standard error (i.e., the estimate of $\sigma$, the standard deviation of the model error term)
-   The residual standard error degrees of freedom (sample size - \# of regression parameters)
-   Number of observations excluded (R automatically deletes any observations with missing data)
-   Multiple $R^2$ (square of the pearson correlation coefficient)
-   Adjusted $R^2$ (penalizes the correlation based on how many predictors are in the model)
-   Global F Test

```{r}
# fit the model 
fit1 <- lm(LBDGLUSI ~ BMXWAIST, data = nhanesf)

# output the results 
summary(fit1)
```
You can also access specific elements of the linear model fit summary using the `$`. A few examples are below. 

```{r}
summary(fit1)$coef
```

```{r}
summary(fit1)$coef["BMXWAIST", "Pr(>|t|)"]
```

The `confint()` function will return confidence intervals for all coefficients in your model. 

```{r}
confint(fit1)
```


## Nice-Looking Summary Table 

The `gtsummary` package can be used to create a more aesthetically pleasing table. You can then export this to a word document.

```{r}
fit1_table <- fit1 %>%
  tbl_regression(intercept = T,
                 estimate_fun = function(x) style_sigfig(x, digits = 4),
                 pvalue_fun   = function(x) style_pvalue(x, digits = 3),
                 label        = list(BMXWAIST ~ "Waist circumference (cm)")) %>%
  modify_caption("Regression of fasting glucose (mmol/L) on waist circumference")
fit1_table
```

```{r}
fit1_table %>%
  as_flex_table() %>%
  flextable::save_as_docx(path = here("03-SimpleLinearRegression", 
                           "03-fit1table.docx"))
```

## Centering and Scaling a Continuous Predictor 

Sometimes it is of interest to center and/or scale the continuous predictor. This is easy to do in R. 

### Centering Only

To only center the continuous predictor, you can create a centered variable "by hand" or use the `scale()` function. We do both below and see the results are equivalent.

```{r}
nhanesf$BMXWAISTc = nhanesf$BMXWAIST - mean(nhanesf$BMXWAIST, na.rm = TRUE)

nhanesf$BMXWAISTc2 = scale(nhanesf$BMXWAIST, scale = FALSE)[,1]

identical(nhanesf$BMXWAISTc, nhanesf$BMXWAISTc2)
```

### Centering and Scaling

The default for the `scale()` function is to both center and scale the variable. 

```{r}
nhanesf$BMXWAISTs = scale(nhanesf$BMXWAIST)[,1]
```


### How this changes the regression coefficients 

We can compare the coefficients from the original fit, the centered fit, and the centered and scaled fit. Centering the predictor only changes the intercept estimate. Scaling the predictor changes the estimate of the predictor, but not the test statistic or p-value. 

```{r}
fit1c = lm(LBDGLUSI ~ BMXWAISTc, data = nhanesf)

fit1s = lm(LBDGLUSI ~ BMXWAISTs, data = nhanesf)

summary(fit1)$coef
summary(fit1c)$coef
summary(fit1s)$coef
```

# Predictions from the Model 

You can use the `predict()` function to output the predicted outcome with a certain level of the predictor. You can include a confidence interval, prediction interval, or neither. Prediction intervals are wider than confidence intervals because they account for the additional uncertainty surrounding an individual prediction, where confidence intervals describe the confidence around the mean. 

```{r}
predict(fit1,
        newdata = data.frame(BMXWAIST = 100),
        interval = "confidence")

predict(fit1,
        newdata = data.frame(BMXWAIST = 100),
        interval = "prediction")
```

We can very easily add the confidence intervals to our scatterplot as shown below. 

```{r, message = FALSE}
ggplot(data = nhanesf, aes(BMXWAIST, LBDGLUSI)) + geom_point() + geom_smooth(method = lm, se = TRUE, alpha = 0.9)
```

Adding the prediction intervals to the scatterplot requires a little more work. You can see the intervals are *much* wider.

```{r, message = FALSE}
# Generate predictions with prediction intervals
pred <- predict(fit1, newdata = nhanesf, interval = "prediction")

# Combine with original data
nhanesf <- cbind(nhanesf, pred)

ggplot(data = nhanesf, aes(BMXWAIST, LBDGLUSI)) + geom_point() + geom_smooth(method = lm, se = FALSE) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.5)
```


# ANOVA Table

The `anova()` function outputs the ANOVA table that summarizes the sums of squares and provides the results from the global F test.

```{r}
anova_fit1 = anova(fit1)
anova_fit1
```

The $R^2$ statistic can also be derived from the ANOVA table as it is the sum of squares of the model divided by the total sum of squares. You can see that this is the same value as from the `lm` output.

```{r}
r2_anova = anova_fit1$`Sum Sq`[1] / sum(anova_fit1$`Sum Sq`)

r2_lm = summary(fit1)$r.squared

r2_anova

r2_lm
```


# Equivalent Tests

The t-test from the `lm` function and the Global F-test (found both in the `lm` function and the `anova` function) are equivalent. The t-test statistic should be the square root of the F statistic.

```{r}
# F statistic from ANOVA
anova_fit1$`F value`[1]

# F statistic from lm
summary(fit1)$fstatistic[1]

# t statistic from lm squared
summary(fit1)$coef["BMXWAIST", "t value"]^2
```


# Residuals 

There are several different types of residuals: 

-   Unstandardized Residuals: Raw difference between the observed and fitted values (use `residuals()`, `resid()`, or `fit1$residuals`)
-   Standardized Residuals: divided by an estimate of their standard deviation (use `rstandard()`)
-   Studentized Residuals: divided by the standard deviation estimated from the regression with that case removed (use`rstudent()`)

```{r}
# compare the three types of residuals

# compare average / quantiles 
residuals_fit1 = data.frame(
  unstandardized_resid = residuals(fit1),
  standardized_resid = rstandard(fit1),
  studentized_resid = rstudent(fit1)
)
summary(residuals_fit1)

# compare standard deviations
apply(residuals_fit1, 2, sd)
```

# Testing Assumptions / Diagnostics

You should check all assumptions made to for the simple linear model and investigate any potential outliers or influential points. 

Assumptions of the Linear Model: 

-   Independence (checked by thinking about how the data were collected)
-   Normality 
-   Linearity
-   Constant Variance 

## Testing the Normality Assumption 

It is recommended to test this assumption visually using QQ plots and histograms of the residuals.

### Q-Q Plot of Residuals  

The closer the points are to the black line, the better the normality assumption is met.

```{r}
ggplot(residuals_fit1, aes(sample = standardized_resid)) +
  stat_qq(size = 2.5, color = 'red') +
  stat_qq_line()
```

### Histogram of Residuals 

In the histogram below, the red solid line provides a smoothed estimate of the distribution of the observed residuals. The blue dashed line is the best fit normal curve based on the mean and standard deviation of the residuals. The closer these two curves are to each other, the better the normality assumption is met.

```{r}
# Create the histogram with the empirical density and normal curve
ggplot(residuals_fit1, aes(x = standardized_resid)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, 
                 fill = "grey", color = "black") +
  geom_density(color = "red", size = 1.5) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals_fit1$standardized_resid, na.rm = TRUE),
                            sd = sd(residuals_fit1$standardized_resid, na.rm = TRUE)), 
                color = "blue", linetype = "dashed", size = 1.5) +
  labs(x = "Residuals", y = "Density")
```

## Testing the Linearity Assumption 

You can check this assumption by plotting a component-plus-residual (CR) plot (partial residual plot). This can be done in R using `crPlots()` in the `car` package.

The raw predictor values are plotted on the horizontal axis, and the vertical axis plots the fitted value minus the intercept (the residual plus the coefficient estimate multiplied by the predictor). The dashed line is the best fit line for the variables on these axes and the solid line is a potentially nonlinear smoother. The closer these two lines are, the better the linearity assumption is met.

```{r}
crPlots(fit1, pch=20, col="gray",
        smooth = list(smoother= gamLine))
```

## Testing the Constant Variance Assumption

You can plot the residuals vs the fitted values to see if the spread of residuals varies across the fitted values. This can be done in R using the `residualPlots()` function in the `car` package. 

```{r}
# fitted = T requests the residual vs. fitted plot
# tests = F and quadratic = F suppresses hypothesis tests
residualPlots(fit1,
              pch=20, col="gray", type = "rstandard", terms = ~ 1, 
              fitted = T, tests = F, quadratic = F)
```

## Outliers

An outlier in simple linear regression is an observation with a very large residual. This can be evaluated by looking at the studentized residuals vs the fitted values. In R, this can be also be done using the `residualPlots()` function and changing the residual type.

```{r}
residualPlots(fit1,
              pch=20, col="gray", type = "rstudent", terms = ~ 1, 
              fitted = T, tests = F, quadratic = F)
```

You can also use an statistical test for outliers using the `outlierTest()` function, which flags observations with unusually large studentized residuals.  

```{r}
outlierTest(fit1, n.max = Inf)
```

## Leverage Points 

Leverage points are data points with unusual predictor values compared to the rest of the data. Leverage values can be calculated using the `hatvalues()` function. 

```{r}
hatvalues(fit1) %>% sort(decreasing = TRUE)
```


## Influential Observations 

Influential observations are ones that alter the regression coefficients by a significant amount when they are included in the model. To test the influence of a point, you can compare the regression coefficient estimates including the point to the ones without including the point in the sample. 

Cook's distance (`cooks.distance()`) is a measure that takes into account the difference in all coefficient estimates when fitting the model with and without the observation in question. Standardized DFBetas (`dfbetas()`) measure the difference for each coefficient estimate separately. The `influenceIndexPlot()` function in the `car` package can plot the Cook's distance, and the `ols_plot_dfbetas()` function in the `olsrr` package can plot the DFBetas.

```{r}
influenceIndexPlot(fit1, vars = "Cook")

ols_plot_dfbetas(fit1)
```

## Functional Form

For the NHANES example, we saw when checking the assumptions that the linearity assumption was not violated. In order to explore other functional forms of the predictor, we will use a different example, from the UNHDD dataset. We want to quantify the effect of female education on adult female mortality. 

First let's fit a simple linear regression model and check the linearity assumption. 

```{r}
# fit the model
fit2 = lm(mort_adult_f ~ educ_f, data = unhdd)

# check linearity assumption
crPlots(fit2, pch=20, col="gray",
        smooth = list(smoother= gamLine))
```

We can see from the plot that the solid pink line looks more quadratic and doesn't quite overlap with the linear one. We will try fitting a model with a quadratic term next. To do so, you must use the syntax `I(educ_f^2)` in addition to the linear term.

```{r}
# fit quadratic model 
fit2_quadratic = lm(mort_adult_f ~ educ_f + I(educ_f^2), 
                    data = unhdd)
summary(fit2_quadratic)
```

```{r}
# re-check linearity assumption
crPlots(fit2_quadratic, pch=20, col="gray",
        smooth = list(smoother= gamLine))
```

# Session information

```{r echo = FALSE, results = 'markup'}
sessionInfo()
```

